%
% TÍTULO DEL CAPÍTULO
%
\chapter{Performance \& Experimental Results
	\label{chapter_4}
}

In this chapter we will delve in the obtained performance results in the two test machines, as well as taking a look at the experimental results obtained for a couple of test datasets. The images that will be used for testing are described in \autoref{dataset_table}. They were obtained from the Middlebury Stereo Vision Page, that offers several test datasets from which we have chosen a couple of them \cite{scharstein2002taxonomy,scharstein2007learning,hirschmuller2007evaluation}.

\begin{table}[h]
\centering
\begin{tabular}{clcc}
\toprule
& Dataset & Width & Height \\
\midrule
 \parbox{2.5cm}{\includegraphics[width=\linewidth]{../data/tsukuba_l.png}} & Tsukuba & 384  & 288 \\[1cm]
 \parbox{2.5cm}{\includegraphics[width=\linewidth]{../data/bowling_l.png}} & Bowling & 1252  & 1110 \\[1cm]
\bottomrule
\end{tabular}
\caption{Datasets used in our tests.}
\label{dataset_table}
\end{table}

\section{Performance Results}

Two computers with different hardware were used for testing. The first, \textbf{Computer 1} has the following specs:

\begin{itemize}
	\item Intel Core i7-3770 CPU (4 cores, 8 threads)
	\item NVIDIA GT 640 graphics card
	\item 16 GB of 1600 MHz DDR3 RAM
	\item WDC WD10EZRX HDD
\end{itemize} 

The second, \textbf{Computer 2} has the following specifications:

\begin{itemize}
	\item Intel Core i7-4930K CPU (6 cores, 12 threads)
	\item NVIDIA GTX 780Ti graphics card
	\item 16 GB of 2133 MHz DDR3 RAM
	\item WD Caviar Black HDD
\end{itemize} 

The operating system used to run the tests was Windows 7 x64, with the VC{}\verb!++! v110 compiler.

\begin{figure}[h]
	\centering
	\subfigure[Computer 1]{\includegraphics[width=0.49\textwidth]{figures/tsu_perf_cpu_3d_1.pdf}} \hfill
	\subfigure[Computer 2]{\includegraphics[width=0.49\textwidth]{figures/tsu_perf_cpu_3d_2.pdf}} \hfill
	\caption{CPU performance results for the Tsukuba dataset.} \label{perf_cpu_3d}
\end{figure} 

The first test performed can be seen in \autoref{perf_cpu_3d}. In it we have used the Tsukuba dataset to test CPU performance with multiple window sizes and maximum disparities. As we can see, the diferences in small images when using a CPU with more cores are negligible; in fact, probably because of cache issues the CPU with more cores fares worse. As we can see, the performance achieved for the more demanding settings are not suitable for real-time applications even with low resolutions like these.

\begin{figure}[h]
	\centering
	\subfigure[Computer 1]{\includegraphics[width=0.49\textwidth]{figures/bow_perf_cpu_3d_1.pdf}} \hfill
	\subfigure[Computer 2]{\includegraphics[width=0.49\textwidth]{figures/bow_perf_cpu_3d_2.pdf}} \hfill
	\caption{CPU performance results for the Bowling dataset.} \label{perf_cpu_3d_bow}
\end{figure} 

Next, in \autoref{perf_cpu_3d_bow}, we have tested the Bowling dataset for the same range of window sizes and maximum disparities. These dataset possesses a higher resolution and we start to see how the faster machine obtains better times than the slower one. But, since we have increased resolution substantially, the timings obtained with this dataset are even less suitable for real-time use cases.

\begin{figure}[h]
	\centering
	\subfigure[Computer 1]{\includegraphics[width=0.49\textwidth]{figures/tsu_perf_ocl_3d_1.pdf}} \hfill
	\subfigure[Computer 2]{\includegraphics[width=0.49\textwidth]{figures/tsu_perf_ocl_3d_2.pdf}} \hfill
	\caption{OpenCL performance results for the Tsukuba dataset.} \label{perf_ocl_3d}
\end{figure} 

Since CPU approaches do not seem like they are going to allow real-time processing of the chosen resolutions, we proceed in \autoref{perf_ocl_3d} to test our OpenCL implementation with the Tsukuba dataset employing the same settings. We instantly see that this implementation improves computation times substantially. With this dataset, even the low end graphics card is able to compute the disparity map in times suitable for real-time applications. \textbf{Computer 2} that has a more powerful graphics processor obtains even better results, achieving further performance gains.

\begin{figure}[h]
	\centering
	\subfigure[Computer 1]{\includegraphics[width=0.49\textwidth]{figures/bow_perf_ocl_3d_1.pdf}} \hfill
	\subfigure[Computer 2]{\includegraphics[width=0.49\textwidth]{figures/bow_perf_ocl_3d_2.pdf}} \hfill
	\caption{OpenCL performance results for the Bowling dataset.} \label{perf_ocl_3d_bow}
\end{figure} 

Next, in \autoref{perf_ocl_3d_bow}, we have performed the same test on the Bowling dataset. This dataset requires a lot more processing power, that is why although the execution times have improved substantially, \textbf{Computer 1} is not obtaining the performance numbers necessary for real-time processing. In contrast, \textbf{Computer 2} does achieve results capable of processing several frames of video per second.

\begin{figure}[h]
	\centering
	\subfigure[Computer 1]{\includegraphics[width=0.49\textwidth]{figures/tsu_perf_cuda_3d_1.pdf}} \hfill
	\subfigure[Computer 2]{\includegraphics[width=0.49\textwidth]{figures/tsu_perf_cuda_3d_2.pdf}} \hfill
	\caption{CUDA performance results for the Tsukuba dataset.} \label{perf_cuda_3d}
\end{figure} 

Once we have seen how the OpenCL implementation performs, the only task left is to test the CUDA module. The first test performed can be seen in \autoref{perf_cuda_3d}, in it we have tested the CUDA implementation with the Tsukuba dataset. The results obtained resemble closely the OpenCL results, in some cases winning and losing by small margins.

\begin{figure}[h]
	\centering
	\subfigure[Computer 1]{\includegraphics[width=0.49\textwidth]{figures/bow_perf_cuda_3d_1.pdf}} \hfill
	\subfigure[Computer 2]{\includegraphics[width=0.49\textwidth]{figures/bow_perf_cuda_3d_2.pdf}} \hfill
	\caption{CUDA performance results for the Bowling dataset.} \label{perf_cuda_3d_bow}
\end{figure} 

The next test performed analyzes the CUDA code with the Bowling dataset (see \autoref{perf_cuda_3d_bow}). Again, the results obtained are quite close to the OpenCL implementation obtaining better or worse results depending on the settings used but with small differences. 

\begin{figure}[h]
	\centering
	\subfigure[Computer 1]{\includegraphics[width=0.49\textwidth]{figures/ocl_cuda_times_1.pdf}} \hfill
	\subfigure[Computer 2]{\includegraphics[width=0.49\textwidth]{figures/ocl_cuda_times_2.pdf}} \hfill
	\caption{OpenCL vs. CUDA in the Bowling dataset.} \label{ocl_vs_cuda}
\end{figure} 

Since execution times for the CUDA and OpenCL implementations are so close, in \autoref{ocl_vs_cuda} we have compared the execution times obtained in both test machines for the most demanding dataset. As mentioned before both implementations are really close in terms of performance.

\begin{figure}[h]
	\centering
	\subfigure[Computer 1]{\includegraphics[width=0.49\textwidth]{figures/speedup_1.pdf}} \hfill
	\subfigure[Computer 2]{\includegraphics[width=0.49\textwidth]{figures/speedup_2.pdf}} \hfill
	\caption{Speedup obtained in the Bowling dataset.} \label{speedup}
\end{figure} 

The last analysis performed using GCVL compares the speedups obtained with the GPU against the CPU implementation of the algorithm. The CPU execution times used to calculate the speedup are from the high end CPU in \textbf{Computer 2}. The speedups obtained with both graphics cards are respectable, but the ones obtained with the \textbf{Computer 2} are specially remarkable, with an average speedup of 77x.

\section{Experimental Results}

\begin{figure}[h]
	\centering
	\subfigure[Resulting depth map]{\includegraphics[width=0.49\textwidth]{figures/tsukuba_dm_5_16.jpg}} \hfill
	\subfigure[Ground truth depth map]{\includegraphics[width=0.49\textwidth]{figures/tsukuba_o_d.jpg}} \hfill
	\caption{Depth map obtained with a maximum disparity of 16 and an aggregation window of 5 in the Tsukuba dataset.} \label{tsukuba_depth_map_1}
\end{figure} 

In order to test the implemented algorithm, we have processed the test images with GCVL. The first results obtained were the depth maps of the Tsukuba dataset, they can be seen in \autoref{tsukuba_depth_map_1} and \autoref{tsukuba_depth_map_2}.

\begin{figure}[h]
	\centering
	\subfigure[Resulting depth map]{\includegraphics[width=0.49\textwidth]{figures/tsukuba_dm_9_16.jpg}} \hfill
		\subfigure[Ground truth depth map]{\includegraphics[width=0.49\textwidth]{figures/tsukuba_o_d.jpg}} \hfill
	\caption{Depth map obtained with a maximum disparity of 16 and an aggregation window of 9 in the Tsukuba dataset.} \label{tsukuba_depth_map_2}
\end{figure} 

As we can see in the tests, as we increase the aggregation window size, the depth map becomes smoother. The main drawback when increasing this parameter, is that a loss of detail occurs, since the pixel window compared is bigger and the estimation is more statistically robust but less coarse.

\begin{figure}[h]
	\centering
	\subfigure[Bowling dataset]{\includegraphics[width=0.49\textwidth]{figures/bowling.png}} \hfill
		\subfigure[Textureless map]{\includegraphics[width=0.49\textwidth]{figures/bowling-textureless-map.png}} \hfill
	\caption{Textureless map of the Bowling dataset, pixels marked as white are low-texture regions.} \label{textureless_regions}
\end{figure}

Moreover, we can see that there are some artifacts in the obtained depth maps. The first reason why these artifacts occur is because of half-occluded (objects in the scene in one image, and not in the other) pixels in the final disparity map. There can also be occluded regions in the left and right images. In addition, there can be regions where there is little or no texture in the scene (a good example of this can be seen in the bowling ball of the Bowling dataset). These can be defined in \cite{scharstein2002taxonomy} as regions where the squared horizontal intensity gradient averaged over a square window of a given size is below a given threshold (see \autoref{textureless_regions}).

\begin{figure}[h]
	\centering
	\subfigure[Resulting depth map]{\includegraphics[width=0.49\textwidth]{figures/bowling_dm_13_100.jpg}} \hfill
	\subfigure[Ground truth depth map]{\includegraphics[width=0.49\textwidth]{figures/bowling_o_d.png}} \hfill
	\caption{Depth map obtained with a maximum disparity of 100 and an aggregation window of 13 in the Bowling dataset.} \label{bowling_depth_map_1}
\end{figure}

We can see some of the aforementioned effects in the Second results obtained using the Bowling dataset. The resulting depth maps can be viewed in \autoref{bowling_depth_map_1} and \autoref{bowling_depth_map_2}.

\begin{figure}[h]
	\centering
	\subfigure[Resulting depth map]{\includegraphics[width=0.49\textwidth]{figures/bowling_dm_13_170.jpg}} \hfill
	\subfigure[Ground truth depth map]{\includegraphics[width=0.49\textwidth]{figures/bowling_o_d.png}} \hfill
	\caption{Depth map obtained with a maximum disparity of 170 and an aggregation window of 13 in the Bowling dataset.} \label{bowling_depth_map_2}
\end{figure}

In the first test we have seen how the aggregation window size influences the smoothness of the disparity map, now we have used a different maximum disparity in each case. If we choose a lower maximum disparity, the algorithm will be faster, but one runs the risk of not finding the corresponding window in the right image. 

In our datasets from the Middlebury Stereo's website, then the answer is simple. The maximum disparity value is the maximum value of the pixel in the ground truth map, divided by the scale factor. If the user has a stereo vision set-up taking real-world imagery, then we will have to do a bit of math to calculate the values. The first equation that we will use is the following:

\begin{equation}
r = \frac{bf}{Nx}
\end{equation}

Being $r$ the range of the object that we are trying to compute, $b$ the distance between the centers of the two cameras, $f$ the focal length of the image sensor, $x$ the pixel size of the sensor and $N$ the maximum disparity value.

Another aspect to account for is the uncertainty in detecting objects at a certain range and the actual range itself. The equation that relates these two variables is the next:

\begin{equation}
\Delta r = \left ( \frac{r^2}{bf}\right)x\Delta N
\end{equation}

Being $\Delta r$ the uncertainty in detecting the object at a certain range and $\Delta N$ the change in disparity value. This means, that for a certain range, we will have some uncertainty in the obtained measurements. For more information the user can check \cite{khaleghi2008improved,khaleghi2008new}.
  